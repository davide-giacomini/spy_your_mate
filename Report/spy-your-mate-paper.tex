\documentclass[12pt, conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{graphicx, wrapfig, subcaption}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[numbers]{natbib} % Manual of natbib at https://gking.harvard.edu/files/natnotes2.pdf
\usepackage[hidelinks]{hyperref}
\usepackage[OT1]{fontenc}
\usepackage{setspace}
\begin{document}

\newcommand{\polimi}{Polytechnic University of Milan}
\newcommand{\deib}{Deparment of Electronics, Information and Bioengineering}

\title{Spy Your Mate\\
{\small Project Report --- 054327 - Wireless Internet $\mid$ 054323 - Internet of Things}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Davide Giacomini}
\IEEEauthorblockA{\textit{\polimi\space(PoliMi)} \\
\textit{\deib}\\
\textit{Milan (MI), Italy} \\
\textit{\today} \\
% giacomini.davide@outlook.com
}
% \\
% \IEEEauthorblockN{Jake Campbell}
% \IEEEauthorblockA{\textit{dept. of Engineering} \\
% \textit{University of Illinois at Chicago (UIC)}\\
% Chicago (IL), USA \\
% jacobpmcampbell@gmail.com}
% \and
% \IEEEauthorblockN{Sem Belay}
% \IEEEauthorblockA{\textit{dept. of Engineering} \\
% \textit{University of Illinois at Chicago (UIC)}\\
% Chicago (IL), USA \\
% sbelay2@uic.edu}
% \\
% \IEEEauthorblockN{Alejandro Dorvall}
% \IEEEauthorblockA{\textit{dept. of Engineering} \\
% \textit{University of Illinois at Chicago (UIC)}\\
% Chicago (IL), USA \\
% adorva2@uic.edu}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}
% \onehalfspacing

\begin{abstract}
    After a global pandemic, video streaming calls have become the normality in every situation. Even though the major providers usually encrypt the traffic, some information can be extracted by the packets that are sent between the parties through the internet. By aggregating the packets of a conversation and analyzing some of their features through a ML algorithm, we can infer if somebody is present in front of the camera or there is nobody.
\end{abstract}

\section{Introduction}
In this report I am going to illustrate the work done for the joint project of Wireless Internet and Internet of Things.

I was required to extract some information from a video call with a provider of my choice. In particular, I was requested to analyze the traffic generated by a webcam call and, from such traffic, identify whether a person or only the background is recorded in the video. I was asked to capture the network traffic and filter all the video call packets. Subsequently, I had to extract some important features from the captured packets, and use those features to train a model of Machine Learning. Finally, I had to predict the presence of a person in the background using the trained model and compare the prediction with the ground truth. For the comparison, I had to use a Node-RED chart.

\section{Method}

The first step was to choose a software for network traffic, hence I chose Wireshark\footnote{\href{https://www.wireshark.org/}{wireshark.org}}. I recorded three vide calls. In the first one I was alone, in the other two I was with another person (different each time). The captures duration is more or less between 300 seconds and 420 seconds. For the video calls I used my laptop (Asus) and my smartphone (Samsung Galaxy S10). The provider I used was Zoom\footnote{\href{https://zoom.us/}{zoom.us}}, as I am comfortable with it.

I needed both the IP addresses and the UDP ports to filter the packets of the video call. In Wireshark there is a convenient window under \texttt{Statistics -> Conversations} which shows you the conversations based on the chosen protocol. Clicking on \texttt{UDP}, I ordered the conversations in order to highlight the one with the highest number of packets (and bytes) transferred. It was easy to see which one is the actual video call, because there is a difference of several orders of magnitude in number of bytes exchanged. The addresses and the ports that I extracted from Wireshark can be seen at Table~\ref{table:table-addresses}.

\begin{table*}[ht]
    \centering
    \begin{tabular}{| c ||c c c c||}
     \hline
      & Zoom IP Address & Zoom UDP Port & Local IP Address & Local UDP Port \\
     \hline\hline
     Capture 1 & 149.137.11.203 & 8801 & 192.168.1.15 & 60235 \\ 
     \hline
     Capture 2 & 149.137.11.142 & 8801 & 192.168.1.15 & 51541 \\
     \hline
     Capture 3 & 149.137.10.223 & 8801 & 192.168.1.15 & 52409 \\ 
     \hline
    \end{tabular}
    \caption{Addresses for each capture.}
    \label{table:table-addresses}
\end{table*}


\subsection{Python Code}
After extracting the addresses, I used them to filter out only the packets regarding the conversation between the parties. I used PyShark\footnote{\href{https://pypi.org/project/pyshark/}{pypi.org/project/pyshark/}} to manipulate the captures. The Python scripts are used to extract the packets, organize them in chunks of half a second, and save the chunks and their features on a csv file, later used for training or testing. The scripts are called in order by the \texttt{prepare\_*.py} scripts. I divided them for convenience during debugging.

The training and testing is done by two scripts called \texttt{script\_ML\_*.py}. The cross validation is only used to understand if the features chosen are good to make an accurate prediction, and the second script is used to actually predict the results. I used the second capture (\texttt{capture2\_test.csv}) to test the model and the other two captures to train it.

I incrementally added features to get a model each time more accurate, and I finally ended up with (TODO NUMBER OF FEATURES) features. Each of them represents a chunk of half a second packets. I am going to describe them below. Notice that each point represents three features, i.e. inbound packets, outbound packets and all packets:
\begin{itemize}
    \item Average packet interval: the average time between one packet and the next.
    \item Average packet size: the average packet size in bytes.
    \item Packets number: the total number of packets.
    \item Bit rate: the number of bytes per half a second.
    \item TODO: more features
\end{itemize}

I used two different algorithms to train the model: logistic regression and random forest. The predictions are saved as \texttt{csv} files to be later used in Node-RED.

\subsection{Node-RED Flow}
In Node-RED I was required to compare the ground truth with the actual prediction done in Python.

In order to do that, I read with Node-RED two different csv files (the capture file exported in csv and the prediction of python) and compared the results with the Chart of Node-RED. For comparing the results, I had to filter out the ground truth taking only the multiples of half a second. The prediction file, instead, had already been prepared with Python.

\subsection{ML Training and Testing}
For the training and the testing of models (both logistic regression and random forest), I divided the work in two main parts. The former one is the cross validation, to understand more or less the accuracy of my predictions, and the latter one is the actual training and prediction with the features I chose during the cross validation. Both the cross validation and the actual training and prediction take as input the \texttt{capture*.csv} files that I prepared with PyShark. I used \texttt{scikit-learn}\footnote{\href{https://scikit-learn.org/stable/}{scikit-learn.org/stable/}} for the ML methods.

The cross validation file (\texttt{script\_ML\_cross\_validation.py}) takes as input the csv files thought for the training part (I chose capture 1 and 3 for the training part) and applies a K fold cross validation with $k=5$. I set the maximum training set size to the 80\% of the training data, hence the minimum validation set is 20\% of the training data. I am basing the terminology on the slide number 5 from the set of slides \cite{matteucci2022neuralnetworkstraining}. After the cross validation, a learning curve shows the training and validation accuracy in function of the number of samples (training size).

After the cross vaildation, we can proceed with the actual training of the model and the subsequent prediction with the trained model. The steps are two: the training and then the testing. For the training, as before, the file takes as input the csv training files and trains the two models (logistic regression and random forest) with those two files. Then, for the testing, the file takes as input the csv file of capture 2 and predicts the presence or absence of a person in front of the webcam. The prediction is then exported in a csv file for Node-RED and is also compared to the ground truth, showing the confusion matrix and the accuracy of the prediction.



\section{Experimental Results}

I ran several trials, but I will report here only the ones that I found interesting for the sake of the document.

\subsection{First attempt}
I started with five features: average packets interval, average packets size, inbound and outbound packets number, total packets number.

\subsubsection{Cross validation results}
\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{resources/learning_curve_basic_features_log_regr.png}
        \caption{Logistic Regression Curve}
        \label{fig:learning-curves-first-attempt-randfor}
    \end{subfigure}%
    \begin{subfigure}[b]{.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{resources/learning_curve_basic_features_rand_for.png}
        \caption{Random Forest Curve}
        \label{fig:learning-curves-first-attempt-logreg}
    \end{subfigure}
    \caption{Learning curves in the first attempt}
    \label{fig:learning-curves-first-attempt}
\end{figure*}

The learning curve of the cross validation for both models can be seen at Fig.~\ref{fig:learning-curves-first-attempt}. Notice that the training curve of the random forest is always equal to 100\% accuracy, and that is not exactly the best behaviour required (Fig.~\ref{fig:learning-curves-first-attempt-randfor}). Nonetheless, the validation accuracy is quite consinstent with the testing results, which I will show later. In particular, the validation accuracy stands at around 70\%. On the other hand, the logistic regression curve is quite ``normal'' (Fig.~\ref{fig:learning-curves-first-attempt-logreg}). The training accuracy is about 77\% and the validation accuracy is about 76\%.

\subsubsection{Training and testing results}


\bibliographystyle{unsrtnat} % Cannot upload \bibliographystyle{IEEEtran}, don't know why
\bibliography{bibliography}

\end{document}
